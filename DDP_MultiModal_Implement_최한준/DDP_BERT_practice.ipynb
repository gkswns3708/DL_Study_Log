{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "\n",
    "from transformers import AutoModel,ViTModel,ViTFeatureExtractor\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import matplotlib as mpl\n",
    "\n",
    "plt.rcParams['font.family'] = 'NanumGothic'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "결과적으로 굉장히 Class Embalance한 Dataset임.\n",
    "\n",
    "Weighted F1 Score더라도,, Embalance Dataset을 위한 Additional한 작업이 요구됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../train.csv')\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['cat3'].values)\n",
    "df['original'] = df['cat3']\n",
    "df['cat3'] = le.transform(df['cat3'].values)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['cat2'].values)\n",
    "df['cat2'] = le.transform(df['cat2'].values)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['cat1'].values)\n",
    "df['cat1'] = le.transform(df['cat1'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "df['kfold'] = -1\n",
    "for i in range(5):\n",
    "    df_idx, valid_idx = list(folds.split(df.values, df['cat3']))[i]\n",
    "    valid = df.iloc[valid_idx]\n",
    "\n",
    "    df.loc[df[df.id.isin(valid.id) == True].index.to_list(), 'kfold'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat3_cnt_dict = defaultdict(int)\n",
    "for value in df['original']:\n",
    "    cat3_cnt_dict[value] += 1\n",
    "cat3_cnt_dict = {k : v for k, v in sorted(cat3_cnt_dict.items(), key=lambda x : x[1], reverse=True)}\n",
    "\n",
    "fig, axes = plt.subplots(1, figsize=(12, 7))\n",
    "\n",
    "x, y = list(cat3_cnt_dict.keys()), list(cat3_cnt_dict.values())\n",
    "axes.bar(x,y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(nn.Module):\n",
    "    def __init__(self, text, image_path, cats1, cats2, cats3, tokenizer, feature_extractor, max_len):\n",
    "        self.text = text\n",
    "        self.image_path = image_path\n",
    "        self.cats1 = cats1\n",
    "        self.cats2 = cats2\n",
    "        self.cats3 = cats3\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor # pretrained LLM (HuggingFace KoBERT)\n",
    "        self.max_len = max_len # 실험을 통해 overview의 max_len을 포함하게 해야할 듯(?)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        image_path = os.path.join('../', str(self.image_path[index][2:]))\n",
    "        image = cv2.imread(image_path)\n",
    "        cat1 = self.cats1[index] # Label1 \n",
    "        cat2 = self.cats2[index] # Label2\n",
    "        cat3 = self.cats3[index] # Label3\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation = True, # max_len 넘으면 자를거냐 (True)\n",
    "            return_attention_mask = True, # Attention_mask return 할거냐 (True)\n",
    "            return_tensors='pt' # Tensor로의 변환을 위해 pt 선택.  tf, np 옵션도 존재\n",
    "        )\n",
    "        image_feature = self.feature_extractor(images=image, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids' : encoding['input_ids'].flatten(), # 이거 왜 flatten() ?\n",
    "            'attention_mask' : encoding['attention_mask'].flatten(),\n",
    "            'pixel_values': image_feature['pixel_values'][0],\n",
    "            'cats1': torch.tensor(cat1, dtype=torch.long),\n",
    "            'cats2': torch.tensor(cat2, dtype=torch.long),\n",
    "            'cats3': torch.tensor(cat3, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, feature_extractor, max_len, batch_size, shuffle_=False):\n",
    "    ds = Custom_Dataset(\n",
    "        text=df.overview.to_numpy(),\n",
    "        image_path = df.img_path.to_numpy(),\n",
    "        cats1=df.cat1.to_numpy(),\n",
    "        cats2=df.cat2.to_numpy(),\n",
    "        cats3=df.cat3.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        feature_extractor = feature_extractor,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        shuffle = shuffle_ # TODO: 나중에 False하고 DistributedSampler를 사용해서 DDP로 구현하는 연습하기.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel,ViTModel,ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "\n",
    "class TourClassifier(nn.Module):\n",
    "  def __init__(self, n_classes1, n_classes2, n_classes3, text_model_name, image_model_name):\n",
    "    super(TourClassifier, self).__init__()\n",
    "    self.text_model = AutoModel.from_pretrained(text_model_name).to(device)\n",
    "    self.image_model = ViTModel.from_pretrained(image_model_name).to(device)\n",
    "    \n",
    "    self.text_model.gradient_checkpointing_enable()  \n",
    "    self.image_model.gradient_checkpointing_enable()  \n",
    "\n",
    "    self.drop = nn.Dropout(p=0.1)\n",
    "\n",
    "    def get_cls(target_size):\n",
    "      return nn.Sequential(\n",
    "          nn.Linear(self.text_model.config.hidden_size, self.text_model.config.hidden_size),\n",
    "          nn.LayerNorm(self.text_model.config.hidden_size),\n",
    "          nn.Dropout(p = 0.1),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(self.text_model.config.hidden_size, target_size),\n",
    "      )  \n",
    "    self.cls = get_cls(n_classes1)\n",
    "    self.cls2 = get_cls(n_classes2)\n",
    "    self.cls3 = get_cls(n_classes3)\n",
    "    \n",
    "  def forward(self, input_ids, attention_mask,pixel_values):\n",
    "    text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    image_output = self.image_model(pixel_values = pixel_values)\n",
    "    # print(text_output.last_hidden_state.size(), image_output.last_hidden_state.size())\n",
    "    concat_outputs = torch.cat([text_output.last_hidden_state, image_output.last_hidden_state],1)\n",
    "    #config hidden size 일치해야함\n",
    "    # Transformer의 Encoding 부분의 Layer가 TransformerEncoderLayer로 구현이 되어 있고, nhead=k를 통해 k번 반복해 Encoding Layer를 구성합니다.\n",
    "    encoder_layer = nn.TransformerEncoderLayer(d_model=self.text_model.config.hidden_size, nhead=8).to(device)\n",
    "    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2).to(device)\n",
    "\n",
    "    outputs = transformer_encoder(concat_outputs)\n",
    "    #cls token \n",
    "    outputs = outputs[:,0] # (Batch, [0]) -> (Batch, [CLS])\n",
    "    output = self.drop(outputs)\n",
    "\n",
    "    out1 = self.cls(output)\n",
    "    out2 = self.cls2(output)\n",
    "    out3 = self.cls3(output)\n",
    "    return out1,out2,out3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "\n",
    "def calc_tour_acc(pred, label):\n",
    "    _, idx = pred.max(1)\n",
    "    \n",
    "    acc = torch.eq(idx, label).sum().item() / idx.size()[0] \n",
    "    x = label.cpu().numpy()\n",
    "    y = idx.cpu().numpy()\n",
    "    f1_acc = f1_score(x, y, average='weighted')\n",
    "    return acc,f1_acc\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import argparse\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples,epoch):\n",
    "  batch_time = AverageMeter()     \n",
    "  data_time = AverageMeter()      \n",
    "  losses = AverageMeter()         \n",
    "  accuracies = AverageMeter()\n",
    "  f1_accuracies = AverageMeter()\n",
    "  \n",
    "  sent_count = AverageMeter()   \n",
    "    \n",
    "\n",
    "  start = end = time.time()\n",
    "\n",
    "  model = model.train()\n",
    "  correct_predictions = 0\n",
    "  for step,d in enumerate(data_loader):\n",
    "    data_time.update(time.time() - end)\n",
    "    batch_size = d[\"input_ids\"].size(0) \n",
    "\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    pixel_values = d['pixel_values'].to(device)\n",
    "    cats1 = d[\"cats1\"].to(device)\n",
    "    cats2 = d[\"cats2\"].to(device)\n",
    "    cats3 = d[\"cats3\"].to(device)\n",
    "\n",
    "    outputs,outputs2,outputs3 = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      pixel_values=pixel_values\n",
    "    )\n",
    "    _, preds = torch.max(outputs3, dim=1)\n",
    "\n",
    "    loss1 = loss_fn(outputs, cats1)\n",
    "    loss2 = loss_fn(outputs2, cats2)\n",
    "    loss3 = loss_fn(outputs3, cats3)\n",
    "\n",
    "    loss = loss1 * 0.05 + loss2 * 0.1 + loss3 * 0.85\n",
    "\n",
    "    correct_predictions += torch.sum(preds == cats3)\n",
    "    losses.update(loss.item(), batch_size)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    sent_count.update(batch_size)\n",
    "    if step % 200 == 0 or step == (len(data_loader)-1):\n",
    "                acc,f1_acc = calc_tour_acc(outputs3, cats3)\n",
    "                accuracies.update(acc, batch_size)\n",
    "                f1_accuracies.update(f1_acc, batch_size)\n",
    "\n",
    "                \n",
    "                print('Epoch: [{0}][{1}/{2}] '\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.val:.3f}({loss.avg:.3f}) '\n",
    "                      'Acc: {acc.val:.3f}({acc.avg:.3f}) '   \n",
    "                      'f1_Acc: {f1_acc.val:.3f}({f1_acc.avg:.3f}) '           \n",
    "                      'sent/s {sent_s:.0f} '\n",
    "                      .format(\n",
    "                      epoch, step+1, len(data_loader),\n",
    "                      data_time=data_time, loss=losses,\n",
    "                      acc=accuracies,\n",
    "                      f1_acc=f1_accuracies,\n",
    "                      remain=timeSince(start, float(step+1)/len(data_loader)),\n",
    "                      sent_s=sent_count.avg/batch_time.avg\n",
    "                      ))\n",
    "\n",
    "  return correct_predictions.double() / n_examples, losses.avg\n",
    "\n",
    "def validate(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  cnt = 0\n",
    "  for d in tqdm(data_loader):\n",
    "    with torch.no_grad():\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      pixel_values = d['pixel_values'].to(device)\n",
    "      cats1 = d[\"cats1\"].to(device)\n",
    "      cats2 = d[\"cats2\"].to(device)\n",
    "      cats3 = d[\"cats3\"].to(device)\n",
    "      outputs,outputs2,outputs3 = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        pixel_values=pixel_values\n",
    "      )\n",
    "      _, preds = torch.max(outputs3, dim=1)\n",
    "      loss1 = loss_fn(outputs, cats1)\n",
    "      loss2 = loss_fn(outputs2, cats2)\n",
    "      loss3 = loss_fn(outputs3, cats3)\n",
    "\n",
    "      loss = loss1 * 0.05 + loss2 * 0.1 + loss3 * 0.85\n",
    "\n",
    "      correct_predictions += torch.sum(preds == cats3)\n",
    "      losses.append(loss.item())\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "      if cnt == 0:\n",
    "        cnt +=1\n",
    "        outputs3_arr = outputs3\n",
    "        cats3_arr = cats3\n",
    "      else:\n",
    "        outputs3_arr = torch.cat([outputs3_arr, outputs3],0)\n",
    "        cats3_arr = torch.cat([cats3_arr, cats3],0)\n",
    "  acc,f1_acc = calc_tour_acc(outputs3_arr, cats3_arr)\n",
    "  return f1_acc, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import argparse\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples,epoch):\n",
    "\n",
    "  batch_time = AverageMeter()     \n",
    "  data_time = AverageMeter()      \n",
    "  losses = AverageMeter()         \n",
    "  accuracies = AverageMeter()\n",
    "  f1_accuracies = AverageMeter()\n",
    "  \n",
    "  sent_count = AverageMeter()   \n",
    "    \n",
    "\n",
    "  start = end = time.time()\n",
    "\n",
    "  model = model.train()\n",
    "  correct_predictions = 0\n",
    "  for step,d in enumerate(data_loader):\n",
    "    data_time.update(time.time() - end)\n",
    "    batch_size = d[\"input_ids\"].size(0) \n",
    "\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    pixel_values = d['pixel_values'].to(device)\n",
    "    cats1 = d[\"cats1\"].to(device)\n",
    "    cats2 = d[\"cats2\"].to(device)\n",
    "    cats3 = d[\"cats3\"].to(device)\n",
    "\n",
    "    outputs,outputs2,outputs3 = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      pixel_values=pixel_values\n",
    "    )\n",
    "    _, preds = torch.max(outputs3, dim=1)\n",
    "\n",
    "    loss1 = loss_fn(outputs, cats1)\n",
    "    loss2 = loss_fn(outputs2, cats2)\n",
    "    loss3 = loss_fn(outputs3, cats3)\n",
    "\n",
    "    loss = loss1 * 0.05 + loss2 * 0.1 + loss3 * 0.85\n",
    "\n",
    "    correct_predictions += torch.sum(preds == cats3)\n",
    "    losses.update(loss.item(), batch_size)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    sent_count.update(batch_size)\n",
    "    if step % 200 == 0 or step == (len(data_loader)-1):\n",
    "                acc,f1_acc = calc_tour_acc(outputs3, cats3)\n",
    "                accuracies.update(acc, batch_size)\n",
    "                f1_accuracies.update(f1_acc, batch_size)\n",
    "\n",
    "                \n",
    "                print('Epoch: [{0}][{1}/{2}] '\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.val:.3f}({loss.avg:.3f}) '\n",
    "                      'Acc: {acc.val:.3f}({acc.avg:.3f}) '   \n",
    "                      'f1_Acc: {f1_acc.val:.3f}({f1_acc.avg:.3f}) '           \n",
    "                      'sent/s {sent_s:.0f} '\n",
    "                      .format(\n",
    "                      epoch, step+1, len(data_loader),\n",
    "                      data_time=data_time, loss=losses,\n",
    "                      acc=accuracies,\n",
    "                      f1_acc=f1_accuracies,\n",
    "                      remain=timeSince(start, float(step+1)/len(data_loader)),\n",
    "                      sent_s=sent_count.avg/batch_time.avg\n",
    "                      ))\n",
    "\n",
    "  return correct_predictions.double() / n_examples, losses.avg\n",
    "\n",
    "def validate(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  cnt = 0\n",
    "  for d in tqdm(data_loader):\n",
    "    with torch.no_grad():\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      pixel_values = d['pixel_values'].to(device)\n",
    "      cats1 = d[\"cats1\"].to(device)\n",
    "      cats2 = d[\"cats2\"].to(device)\n",
    "      cats3 = d[\"cats3\"].to(device)\n",
    "      outputs,outputs2,outputs3 = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        pixel_values=pixel_values\n",
    "      )\n",
    "      _, preds = torch.max(outputs3, dim=1)\n",
    "      loss1 = loss_fn(outputs, cats1)\n",
    "      loss2 = loss_fn(outputs2, cats2)\n",
    "      loss3 = loss_fn(outputs3, cats3)\n",
    "\n",
    "      loss = loss1 * 0.05 + loss2 * 0.1 + loss3 * 0.85\n",
    "\n",
    "      correct_predictions += torch.sum(preds == cats3)\n",
    "      losses.append(loss.item())\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "      if cnt == 0:\n",
    "        cnt +=1\n",
    "        outputs3_arr = outputs3\n",
    "        cats3_arr = cats3\n",
    "      else:\n",
    "        outputs3_arr = torch.cat([outputs3_arr, outputs3],0)\n",
    "        cats3_arr = torch.cat([cats3_arr, cats3],0)\n",
    "  acc,f1_acc = calc_tour_acc(outputs3_arr, cats3_arr)\n",
    "  return f1_acc, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df[\"kfold\"] != 0].reset_index(drop=True)\n",
    "valid = df[df[\"kfold\"] == 0].reset_index(drop=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-small\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "train_data_loader = create_data_loader(train, tokenizer, feature_extractor, 256, 16, shuffle_=True)\n",
    "valid_data_loader = create_data_loader(valid, tokenizer, feature_extractor, 256, 16)\n",
    "\n",
    "\n",
    "EPOCHS = 2\n",
    "model = TourClassifier(n_classes1 = 6, n_classes2 = 18, n_classes3 = 128, text_model_name = \"klue/roberta-small\",image_model_name = \"google/vit-base-patch16-224\").to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr= 3e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(total_steps*0.1),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, 2)\n",
    "\n",
    "fig.suptitle('Multiple plots')\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & DataLoader 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "train_features = next(iter(train_data_loader))\n",
    "for key, value in train_features.items():\n",
    "    print(key, value.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    print('-' * 10)\n",
    "    print(f'Epoch {epoch}/{EPOCHS-1}')\n",
    "    print('-' * 10)\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(train),\n",
    "        epoch\n",
    "    )\n",
    "    validate_acc, validate_loss = validate(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(valid)\n",
    "    )\n",
    "\n",
    "    if validate_acc > max_acc:\n",
    "        max_acc = validate_acc\n",
    "        torch.save(model.state_dict(),f'tourbaseline_fold0.pt')\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    print(f'Validate loss {validate_loss} accuracy {validate_acc}')\n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ec1e20c32787232cf8ee9fc6c63975afaaae906b20696bd46d2335c222b4549"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
