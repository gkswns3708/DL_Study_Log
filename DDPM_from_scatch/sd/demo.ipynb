{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "x_dhQfFYXoPu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb ì…€ 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m seed \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     output_image \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m         uncond_prompt\u001b[39m=\u001b[39;49muncond_prompt,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m         input_image\u001b[39m=\u001b[39;49minput_image,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m         strength\u001b[39m=\u001b[39;49mstrength,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m         do_cfg\u001b[39m=\u001b[39;49mdo_cfg,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m         cfg_scale\u001b[39m=\u001b[39;49mcfg_scale,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m         sampler_name\u001b[39m=\u001b[39;49msampler,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m         n_inference_steps\u001b[39m=\u001b[39;49mnum_inference_steps,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m         models\u001b[39m=\u001b[39;49mmodels,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m         device\u001b[39m=\u001b[39;49mDEVICE,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m         idle_device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m         tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# Combine the input image and the output image into a single image.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f4844227d@ssh-remote%2B117.16.94.218/data/DL_Study_Log/DDPM_from_scatch/sd/demo.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m Image\u001b[39m.\u001b[39mfromarray(output_image)\n",
            "File \u001b[0;32m/data/DL_Study_Log/DDPM_from_scatch/sd/pipeline.py:60\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt, uncond_prompt, input_image, strength, do_cfg, cfg_scale, sampler_name, n_inference_steps, models, seed, device, idle_device, tokenizer)\u001b[0m\n\u001b[1;32m     57\u001b[0m cond_context \u001b[39m=\u001b[39m clip(cond_tokens)\n\u001b[1;32m     59\u001b[0m \u001b[39m# if we don't want specify, we will use the empty string\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m uncond_tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mbatch_encode_plus((uncond_prompt), padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39m77\u001b[39;49m)\u001b[39m.\u001b[39minput_ids\n\u001b[1;32m     61\u001b[0m uncond_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(uncond_tokens, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     62\u001b[0m \u001b[39m# (Batch_Size, Seq_Len) -> (Batch_Size, Seq_Len, Dim(=768))\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3075\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3065\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3066\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3067\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   3068\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3072\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3073\u001b[0m )\n\u001b[0;32m-> 3075\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   3076\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   3077\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3078\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   3079\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   3080\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3081\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3082\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3083\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3084\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3085\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3086\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3087\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3088\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3089\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3090\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3091\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3092\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3093\u001b[0m )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:807\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m     second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n\u001b[0;32m--> 807\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_prepare_for_model(\n\u001b[1;32m    808\u001b[0m     input_ids,\n\u001b[1;32m    809\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    810\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    811\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    812\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    813\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    814\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    815\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    816\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    817\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    818\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    819\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    820\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    821\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    822\u001b[0m )\n\u001b[1;32m    824\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:879\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    876\u001b[0m             batch_outputs[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    877\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[0;32m--> 879\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    880\u001b[0m     batch_outputs,\n\u001b[1;32m    881\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding_strategy\u001b[39m.\u001b[39;49mvalue,\n\u001b[1;32m    882\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    883\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    884\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    885\u001b[0m )\n\u001b[1;32m    887\u001b[0m batch_outputs \u001b[39m=\u001b[39m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39mreturn_tensors)\n\u001b[1;32m    889\u001b[0m \u001b[39mreturn\u001b[39;00m batch_outputs\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3214\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3212\u001b[0m \u001b[39m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoded_inputs:\n\u001b[0;32m-> 3214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3215\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3216\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat includes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but you provided \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(encoded_inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3217\u001b[0m     )\n\u001b[1;32m   3219\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[1;32m   3221\u001b[0m \u001b[39mif\u001b[39;00m required_input \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(required_input, Sized) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(required_input) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
            "\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []"
          ]
        }
      ],
      "source": [
        "import model_loader\n",
        "import pipeline\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from transformers import CLIPTokenizer\n",
        "import torch\n",
        "\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "ALLOW_CUDA = True\n",
        "ALLOW_MPS = False\n",
        "\n",
        "if torch.cuda.is_available() and ALLOW_CUDA:\n",
        "    DEVICE = \"cuda\"\n",
        "elif (torch.has_mps or torch.backends.mps.is_available()) and ALLOW_MPS:\n",
        "    DEVICE = \"mps\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "tokenizer = CLIPTokenizer(\"../data/vocab.json\", merges_file=\"../data/merges.txt\")\n",
        "model_file = \"../data/v1-5-pruned-emaonly.ckpt\"\n",
        "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
        "\n",
        "## TEXT TO IMAGE\n",
        "\n",
        "# prompt = \"A dog with sunglasses, wearing comfy hat, looking at camera, highly detailed, ultra sharp, cinematic, 100mm lens, 8k resolution.\"\n",
        "prompt = \"A joyful golden retriever puppy sitting on the grass, surrounded by orange flowers, enjoying a sunny day\"\n",
        "uncond_prompt = \"\"  # Also known as negative prompt\n",
        "do_cfg = True\n",
        "cfg_scale = 8  # min: 1, max: 14\n",
        "\n",
        "## IMAGE TO IMAGE\n",
        "\n",
        "input_image = None\n",
        "# Comment to disable image to image\n",
        "image_path = \"../images/dog.jpeg\"\n",
        "input_image = Image.open(image_path)\n",
        "# Higher values means more noise will be added to the input image, so the result will further from the input image.\n",
        "# Lower values means less noise is added to the input image, so output will be closer to the input image.\n",
        "strength = 0.9\n",
        "\n",
        "## SAMPLER\n",
        "\n",
        "sampler = \"ddpm\"\n",
        "num_inference_steps = 50\n",
        "seed = 42\n",
        "\n",
        "output_image = pipeline.generate(\n",
        "    prompt=prompt,\n",
        "    uncond_prompt=uncond_prompt,\n",
        "    input_image=input_image,\n",
        "    strength=strength,\n",
        "    do_cfg=do_cfg,\n",
        "    cfg_scale=cfg_scale,\n",
        "    sampler_name=sampler,\n",
        "    n_inference_steps=num_inference_steps,\n",
        "    seed=seed,\n",
        "    models=models,\n",
        "    device=DEVICE,\n",
        "    idle_device=\"cpu\",\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Combine the input image and the output image into a single image.\n",
        "Image.fromarray(output_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "iDI2dKfRWTId"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
