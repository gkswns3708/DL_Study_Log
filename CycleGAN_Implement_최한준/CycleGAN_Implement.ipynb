{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import argparse\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1), # channel = 1 or gray-scale * 3 => 3\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, padding=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        x = F.avg_pool2d(x, x.size()[2:]) # TODO : 이거 왜 이런지 생각 및 확인하기\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.res = nn.Sequential(nn.ReflectionPad2d(1),\n",
    "                                 nn.Conv2d(in_channels, in_channels, 3),\n",
    "                                 nn.InstanceNorm2d(in_channels),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.ReflectionPad2d(1),\n",
    "                                 nn.Conv2d(in_channels, in_channels, 3),\n",
    "                                 nn.InstanceNorm2d(in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.res(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Initial convolution block\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(3, 64, 7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Downsampling\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Residual blocks\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "\n",
    "            # Upsampling\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Output layer\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, 3, 7),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecayLR:\n",
    "    def __init__(self, epochs, offset, decay_epochs):\n",
    "        epoch_flag = epochs - decay_epochs\n",
    "        assert (epoch_flag > 0), \"Decay must start before the training session ends!\"\n",
    "        self.epochs = epochs\n",
    "        self.offset = offset\n",
    "        self.decay_epochs = decay_epochs\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_epochs) / (\n",
    "                self.epochs - self.decay_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, unaligned=False, mode=\"train\"):\n",
    "        self.transform = transform\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob(os.path.join(root, f\"{mode}/A\") + \"/*.*\"))\n",
    "        self.files_B = sorted(glob(os.path.join(root, f\"{mode}/B\") + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n",
    "\n",
    "        if self.unaligned:\n",
    "            item_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]))\n",
    "        else:\n",
    "            item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))\n",
    "\n",
    "        return {\"A\": item_A, \"B\": item_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, cuda=False, dataroot='./datasets', dataset='horse2zebra', decay_epochs=1, epochs=2, image_size=256, lr=0.0002, manualSeed=None, netD_A='', netD_B='', netG_A2B='', netG_B2A='', outf='./outputs', print_freq=100)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"PyTorch implements `Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks`\")\n",
    "parser.add_argument(\"--dataroot\", type=str, default=\"./datasets\",\n",
    "                    help=\"path to datasets. (default:./data)\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"horse2zebra\",\n",
    "                    help=\"dataset name. (default:`horse2zebra`)\"\n",
    "                         \"Option: [apple2orange, summer2winter_yosemite, horse2zebra, monet2photo, \"\n",
    "                         \"cezanne2photo, ukiyoe2photo, vangogh2photo, maps, facades, selfie2anime, \"\n",
    "                         \"iphone2dslr_flower, ae_photos, ]\")\n",
    "parser.add_argument(\"--epochs\", default=2, type=int, metavar=\"N\", \n",
    "                    help=\"number of total epochs to run\")\n",
    "parser.add_argument(\"--decay_epochs\", type=int, default=1,\n",
    "                    help=\"epoch to start linearly decaying the learning rate to 0. (default:100)\")\n",
    "parser.add_argument(\"-b\", \"--batch-size\", default=1, type=int,\n",
    "                    metavar=\"N\",\n",
    "                    help=\"mini-batch size (default: 1), this is the total \"\n",
    "                         \"batch size of all GPUs on the current node when \"\n",
    "                         \"using Data Parallel or Distributed Data Parallel\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002,\n",
    "                    help=\"learning rate. (default:0.0002)\")\n",
    "parser.add_argument(\"-p\", \"--print-freq\", default=100, type=int,\n",
    "                    metavar=\"N\", help=\"print frequency. (default:100)\")\n",
    "parser.add_argument(\"--cuda\", action=\"store_true\", help=\"Enables cuda\")\n",
    "parser.add_argument(\"--netG_A2B\", default=\"\", help=\"path to netG_A2B (to continue training)\")\n",
    "parser.add_argument(\"--netG_B2A\", default=\"\", help=\"path to netG_B2A (to continue training)\")\n",
    "parser.add_argument(\"--netD_A\", default=\"\", help=\"path to netD_A (to continue training)\")\n",
    "parser.add_argument(\"--netD_B\", default=\"\", help=\"path to netD_B (to continue training)\")\n",
    "parser.add_argument(\"--image-size\", type=int, default=256,\n",
    "                    help=\"size of the data crop (squared assumed). (default:256)\")\n",
    "parser.add_argument(\"--outf\", default=\"./outputs\",\n",
    "                    help=\"folder to output images. (default:`./outputs`).\")\n",
    "parser.add_argument(\"--manualSeed\", type=int,\n",
    "                    help=\"Seed for initializing training. (default:none)\")\n",
    "\n",
    "args = parser.parse_args('')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  7051\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.makedirs(args.outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(\"weights\")\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "if args.manualSeed is None:\n",
    "    args.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", args.manualSeed)\n",
    "random.seed(args.manualSeed)\n",
    "torch.manual_seed(args.manualSeed)\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "if torch.cuda.is_available() and not args.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  \"\"\"\n",
      "/opt/anaconda3/envs/ML/lib/python3.7/site-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  \"Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. \"\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "dataset = ImageDataset(root=os.path.join(args.dataroot, args.dataset),\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize(\n",
    "                               int(args.image_size * 1.12), Image.BICUBIC),\n",
    "                           transforms.RandomCrop(args.image_size),\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "                       unaligned=True)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(os.path.join(args.outf, args.dataset, \"A\"))\n",
    "    os.makedirs(os.path.join(args.outf, args.dataset, \"B\"))\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(os.path.join(\"weights\", args.dataset))\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
    "\n",
    "# create model\n",
    "netG_A2B = Generator().to(device)\n",
    "netG_B2A = Generator().to(device)\n",
    "netD_A = Discriminator().to(device)\n",
    "netD_B = Discriminator().to(device)\n",
    "\n",
    "netG_A2B.apply(weights_init)\n",
    "netG_B2A.apply(weights_init)\n",
    "netD_A.apply(weights_init)\n",
    "netD_B.apply(weights_init)\n",
    "\n",
    "\n",
    "if args.netG_A2B != \"\":\n",
    "    netG_A2B.load_state_dict(torch.load(args.netG_A2B))\n",
    "if args.netG_B2A != \"\":\n",
    "    netG_B2A.load_state_dict(torch.load(args.netG_B2A))\n",
    "if args.netD_A != \"\":\n",
    "    netD_A.load_state_dict(torch.load(args.netD_A))\n",
    "if args.netD_B != \"\":\n",
    "    netD_B.load_state_dict(torch.load(args.netD_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function (adversarial_loss) and optimizer\n",
    "cycle_loss = torch.nn.L1Loss().to(device)\n",
    "identity_loss = torch.nn.L1Loss().to(device)\n",
    "adversarial_loss = torch.nn.MSELoss().to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
    "                               lr=args.lr, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "\n",
    "lr_lambda = DecayLR(args.epochs, 0, args.decay_epochs).step\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lr_lambda)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lr_lambda)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lr_lambda)\n",
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "identity_losses = []\n",
    "gan_losses = []\n",
    "cycle_losses = []\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f8efb17b6743f1994fa4ede809925c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4n/t7xg8fhn1rlgpmh2cv0f210w0000gn/T/ipykernel_2165/1858035988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mloss_cycle_ABA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcycle_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecovered_image_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_image_A\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mrecovered_image_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetG_A2B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_image_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mloss_cycle_BAB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcycle_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecovered_image_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_image_B\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4n/t7xg8fhn1rlgpmh2cv0f210w0000gn/T/ipykernel_2165/3331769492.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4n/t7xg8fhn1rlgpmh2cv0f210w0000gn/T/ipykernel_2165/2722813497.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(0, args.epochs):\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for i, data in progress_bar:\n",
    "        # get batch size data\n",
    "        real_image_A = data[\"A\"].to(device)\n",
    "        real_image_B = data[\"B\"].to(device)\n",
    "        batch_size = real_image_A.size(0)\n",
    "\n",
    "        # real data label is 1, fake data label is 0.\n",
    "        real_label = torch.full((batch_size, 1), 1, device=device, dtype=torch.float32)\n",
    "        fake_label = torch.full((batch_size, 1), 0, device=device, dtype=torch.float32)\n",
    "\n",
    "        ##############################################\n",
    "        # (1) Update G network: Generators A2B and B2A\n",
    "        ##############################################\n",
    "\n",
    "        # Set G_A and G_B's gradients to zero\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        # G_B2A(A) should equal A if real A is fed\n",
    "        identity_image_A = netG_B2A(real_image_A)\n",
    "        loss_identity_A = identity_loss(identity_image_A, real_image_A) * 5.0\n",
    "        # G_A2B(B) should equal B if real B is fed\n",
    "        identity_image_B = netG_A2B(real_image_B)\n",
    "        loss_identity_B = identity_loss(identity_image_B, real_image_B) * 5.0\n",
    "\n",
    "        # GAN loss\n",
    "        # GAN loss D_A(G_A(A))\n",
    "        fake_image_A = netG_B2A(real_image_B)\n",
    "        fake_output_A = netD_A(fake_image_A)\n",
    "        loss_GAN_B2A = adversarial_loss(fake_output_A, real_label)\n",
    "        # GAN loss D_B(G_B(B))\n",
    "        fake_image_B = netG_A2B(real_image_A)\n",
    "        fake_output_B = netD_B(fake_image_B)\n",
    "        loss_GAN_A2B = adversarial_loss(fake_output_B, real_label)\n",
    "\n",
    "        # Cycle loss\n",
    "        recovered_image_A = netG_B2A(fake_image_B)\n",
    "        loss_cycle_ABA = cycle_loss(recovered_image_A, real_image_A) * 10.0\n",
    "\n",
    "        recovered_image_B = netG_A2B(fake_image_A)\n",
    "        loss_cycle_BAB = cycle_loss(recovered_image_B, real_image_B) * 10.0\n",
    "\n",
    "        # Combined loss and calculate gradients\n",
    "        errG = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "\n",
    "        # Calculate gradients for G_A and G_B\n",
    "        errG.backward()\n",
    "        # Update G_A and G_B's weights\n",
    "        optimizer_G.step()\n",
    "\n",
    "        ##############################################\n",
    "        # (2) Update D network: Discriminator A\n",
    "        ##############################################\n",
    "\n",
    "        # Set D_A gradients to zero\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real A image loss\n",
    "        real_output_A = netD_A(real_image_A)\n",
    "        errD_real_A = adversarial_loss(real_output_A, real_label)\n",
    "\n",
    "        # Fake A image loss\n",
    "        fake_image_A = fake_A_buffer.push_and_pop(fake_image_A)\n",
    "        fake_output_A = netD_A(fake_image_A.detach())\n",
    "        errD_fake_A = adversarial_loss(fake_output_A, fake_label)\n",
    "\n",
    "        # Combined loss and calculate gradients\n",
    "        errD_A = (errD_real_A + errD_fake_A) / 2\n",
    "\n",
    "        # Calculate gradients for D_A\n",
    "        errD_A.backward()\n",
    "        # Update D_A weights\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        ##############################################\n",
    "        # (3) Update D network: Discriminator B\n",
    "        ##############################################\n",
    "\n",
    "        # Set D_B gradients to zero\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real B image loss\n",
    "        real_output_B = netD_B(real_image_B)\n",
    "        errD_real_B = adversarial_loss(real_output_B, real_label)\n",
    "\n",
    "        # Fake B image loss\n",
    "        fake_image_B = fake_B_buffer.push_and_pop(fake_image_B)\n",
    "        fake_output_B = netD_B(fake_image_B.detach())\n",
    "        errD_fake_B = adversarial_loss(fake_output_B, fake_label)\n",
    "\n",
    "        # Combined loss and calculate gradients\n",
    "        errD_B = (errD_real_B + errD_fake_B) / 2\n",
    "\n",
    "        # Calculate gradients for D_B\n",
    "        errD_B.backward()\n",
    "        # Update D_B weights\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        progress_bar.set_description(\n",
    "            f\"[{epoch}/{args.epochs - 1}][{i}/{len(dataloader) - 1}] \"\n",
    "            f\"Loss_D: {(errD_A + errD_B).item():.4f} \"\n",
    "            f\"Loss_G: {errG.item():.4f} \"\n",
    "            f\"Loss_G_identity: {(loss_identity_A + loss_identity_B).item():.4f} \"\n",
    "            f\"loss_G_GAN: {(loss_GAN_A2B + loss_GAN_B2A).item():.4f} \"\n",
    "            f\"loss_G_cycle: {(loss_cycle_ABA + loss_cycle_BAB).item():.4f}\")\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            vutils.save_image(real_image_A,\n",
    "                              f\"{args.outf}/{args.dataset}/A/real_samples_epoch_{epoch}_{i}.png\",\n",
    "                              normalize=True)\n",
    "            vutils.save_image(real_image_B,\n",
    "                              f\"{args.outf}/{args.dataset}/B/real_samples_epoch_{epoch}_{i}.png\",\n",
    "                              normalize=True)\n",
    "\n",
    "            fake_image_A = 0.5 * (netG_B2A(real_image_B).data + 1.0)\n",
    "            fake_image_B = 0.5 * (netG_A2B(real_image_A).data + 1.0)\n",
    "\n",
    "            vutils.save_image(fake_image_A.detach(),\n",
    "                              f\"{args.outf}/{args.dataset}/A/fake_samples_epoch_{epoch}_{i}.png\",\n",
    "                              normalize=True)\n",
    "            vutils.save_image(fake_image_B.detach(),\n",
    "                              f\"{args.outf}/{args.dataset}/B/fake_samples_epoch_{epoch}_{i}.png\",\n",
    "                              normalize=True)\n",
    "\n",
    "    # do check pointing\n",
    "    torch.save(netG_A2B.state_dict(), f\"weights/{args.dataset}/netG_A2B_epoch_{epoch}.pth\")\n",
    "    torch.save(netG_B2A.state_dict(), f\"weights/{args.dataset}/netG_B2A_epoch_{epoch}.pth\")\n",
    "    torch.save(netD_A.state_dict(), f\"weights/{args.dataset}/netD_A_epoch_{epoch}.pth\")\n",
    "    torch.save(netD_B.state_dict(), f\"weights/{args.dataset}/netD_B_epoch_{epoch}.pth\")\n",
    "\n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "# save last check pointing\n",
    "torch.save(netG_A2B.state_dict(), f\"weights/{args.dataset}/netG_A2B.pth\")\n",
    "torch.save(netG_B2A.state_dict(), f\"weights/{args.dataset}/netG_B2A.pth\")\n",
    "torch.save(netD_A.state_dict(), f\"weights/{args.dataset}/netD_A.pth\")\n",
    "torch.save(netD_B.state_dict(), f\"weights/{args.dataset}/netD_B.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19d29624fa02f72a2f2eb64b5fa4dfbc751609e2b6c88be691c0db207c64cc14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
